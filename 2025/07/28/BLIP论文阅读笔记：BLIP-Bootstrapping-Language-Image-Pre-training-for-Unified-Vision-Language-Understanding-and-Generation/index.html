<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="论文标题： BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 作者： Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi (Salesforce Research) 论文链接： arXiv:220">
<meta property="og:type" content="article">
<meta property="og:title" content="BLIP论文阅读笔记：BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation">
<meta property="og:url" content="http://example.com/2025/07/28/BLIP%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ABLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/index.html">
<meta property="og:site_name" content="QianQing&#39;s Blog">
<meta property="og:description" content="论文标题： BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation 作者： Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi (Salesforce Research) 论文链接： arXiv:220">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428175825553.png">
<meta property="og:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428095130729.png">
<meta property="og:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250427234034408.png">
<meta property="og:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428170422257.png">
<meta property="og:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428173118623.png">
<meta property="article:published_time" content="2025-07-28T08:45:30.000Z">
<meta property="article:modified_time" content="2025-07-28T09:50:36.532Z">
<meta property="article:author" content="千顷QianQing">
<meta property="article:tag" content="BLIP">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428175825553.png">


<link rel="canonical" href="http://example.com/2025/07/28/BLIP%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ABLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2025/07/28/BLIP%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ABLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/","path":"2025/07/28/BLIP论文阅读笔记：BLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/","title":"BLIP论文阅读笔记：BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>BLIP论文阅读笔记：BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | QianQing's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">QianQing's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%8F%90%E5%87%BABLIP%EF%BC%9F"><span class="nav-number">1.</span> <span class="nav-text">1. 为什么提出BLIP？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%EF%BC%9AMED-Multimodal-Mixture-of-Encoder-Decoder"><span class="nav-number">2.</span> <span class="nav-text">2. 模型结构：MED (Multimodal Mixture of Encoder-Decoder)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E7%BB%86%E8%8A%82%E4%B8%8E%E5%AE%9E%E7%8E%B0%E7%BB%84%E4%BB%B6"><span class="nav-number">2.1.</span> <span class="nav-text">结构细节与实现组件</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%9B%BE%E5%83%8F%E7%BC%96%E7%A0%81%E5%99%A8"><span class="nav-number">2.1.1.</span> <span class="nav-text">(1) 图像编码器</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-Text-Transformer"><span class="nav-number">2.1.2.</span> <span class="nav-text">(2) Text Transformer</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E4%B8%89%E7%A7%8D%E6%A8%A1%E5%BC%8F%E5%AF%B9%E5%BA%94%E7%89%B9%E5%BE%81"><span class="nav-number">2.1.3.</span> <span class="nav-text">(3) 三种模式对应特征</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%80%E4%BA%9B%E7%BB%86%E8%8A%82"><span class="nav-number">2.2.</span> <span class="nav-text">一些细节</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E6%95%B0%E6%8D%AE%E6%96%B9%E6%B3%95%EF%BC%9ACapFilt"><span class="nav-number">3.</span> <span class="nav-text">3. 数据方法：CapFilt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E9%A2%84%E8%AE%AD%E7%BB%83%E7%9B%AE%E6%A0%87"><span class="nav-number">4.</span> <span class="nav-text">4. 预训练目标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-Image-Text-Contrastive-Loss-ITC"><span class="nav-number">4.1.</span> <span class="nav-text">(1) Image-Text Contrastive Loss (ITC)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-Image-Text-Matching-Loss-ITM"><span class="nav-number">4.2.</span> <span class="nav-text">(2) Image-Text Matching Loss (ITM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-Language-Modeling-Loss-LM"><span class="nav-number">4.3.</span> <span class="nav-text">(3) Language Modeling Loss (LM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#4-%E4%B8%89%E4%B8%AALoss%E8%81%94%E5%90%88%E8%AE%AD%E7%BB%83"><span class="nav-number">4.4.</span> <span class="nav-text">(4) 三个Loss联合训练</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C"><span class="nav-number">5.</span> <span class="nav-text">5. 实验结果</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6-%E6%88%91%E7%9A%84%E7%90%86%E8%A7%A3"><span class="nav-number">6.</span> <span class="nav-text">6. 我的理解</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="千顷QianQing"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">千顷QianQing</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">102</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">17</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">44</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/qianqing26" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qianqing26" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:eevee_hbc@outlook.com" title="E-Mail → mailto:eevee_hbc@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/28/BLIP%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9ABLIP-Bootstrapping-Language-Image-Pre-training-for-Unified-Vision-Language-Understanding-and-Generation/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="千顷QianQing">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QianQing's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="BLIP论文阅读笔记：BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | QianQing's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BLIP论文阅读笔记：BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>
      

      <time title="Created: 2025-07-28 16:45:30 / Modified: 17:50:36" itemprop="dateCreated datePublished" datetime="2025-07-28T16:45:30+08:00">2025-07-28</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%A0%94%E9%9B%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">研零学习记录</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p><strong>论文标题：</strong> <em>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</em><br> <strong>作者：</strong> Junnan Li, Dongxu Li, Caiming Xiong, Steven Hoi (Salesforce Research)<br> <strong>论文链接：</strong> <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.12086">arXiv:2201.12086</a></p>
<hr>
<p><img src="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428175825553.png" alt=""></p>
<span id="more"></span>
<h2 id="1-为什么提出BLIP？"><a href="#1-为什么提出BLIP？" class="headerlink" title="1. 为什么提出BLIP？"></a>1. 为什么提出BLIP？</h2><p>当前Vision-Language Pre-training (VLP)方法存在两类主要问题：</p>
<ol>
<li><strong>模型问题：</strong><ul>
<li>Encoder-only模型（如CLIP、ALBEF）在理解任务上强，但难以直接迁移到生成任务。</li>
<li>Encoder-Decoder模型（如SimVLM）适合生成任务，却在检索等理解任务上表现不足。</li>
</ul>
</li>
<li><strong>数据问题：</strong><ul>
<li>大多数VLP依赖web爬取的图文对，这些alt-text噪声大、信息不足。</li>
</ul>
</li>
</ol>
<p>BLIP的目标：</p>
<ul>
<li>构建<strong>统一模型</strong>，兼顾理解任务与生成任务。</li>
<li>在无需大幅增加人工标注的情况下，通过自动处理，让海量web数据更干净、更有信息量。</li>
</ul>
<hr>
<h2 id="2-模型结构：MED-Multimodal-Mixture-of-Encoder-Decoder"><a href="#2-模型结构：MED-Multimodal-Mixture-of-Encoder-Decoder" class="headerlink" title="2. 模型结构：MED (Multimodal Mixture of Encoder-Decoder)"></a>2. 模型结构：MED (Multimodal Mixture of Encoder-Decoder)</h2><p><img src="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428095130729.png" alt=""></p>
<p>BLIP提出<strong>MED架构</strong>，是一个统一的多模态Transformer架构，一个能使用同一套参数在不同任务下切换模式的统一模型：</p>
<ul>
<li><strong>Unimodal Encoder：</strong> 用于Image-Text Contrastive (ITC)任务，分别编码图像和文本，对齐特征空间。</li>
<li><strong>Image-grounded Text Encoder：</strong> 在文本Transformer中插入cross-attention，做Image-Text Matching (ITM)。</li>
<li><strong>Image-grounded Text Decoder：</strong> 将双向自注意力改为因果自注意力，结合cross-attention生成文本（Language Modeling, LM）。</li>
</ul>
<p>特点：</p>
<ul>
<li><strong>一套Transformer参数共享</strong>，根据任务切换mask和结构。</li>
<li><strong>ViT作为图像编码器</strong>，BERT结构作为文本基底。</li>
<li><strong>高效性</strong>：减少参数冗余，多任务共享学习。</li>
</ul>
<p><strong>Mixture的含义：</strong></p>
<ul>
<li>一套Transformer → 三种前向路径 → 不同mask/不同special tokens/是否开启cross-attention → 实现理解+生成任务统一。</li>
</ul>
<h3 id="结构细节与实现组件"><a href="#结构细节与实现组件" class="headerlink" title="结构细节与实现组件"></a>结构细节与实现组件</h3><h4 id="1-图像编码器"><a href="#1-图像编码器" class="headerlink" title="(1) 图像编码器"></a>(1) 图像编码器</h4><ul>
<li><strong>ViT</strong>结构：Patch embedding → Transformer → <code>[CLS]</code>特征 + Patch特征</li>
<li>输出：<ul>
<li><code>[CLS]</code> → 图像全局特征</li>
<li>Patch特征序列 → 提供给Cross-Attention</li>
</ul>
</li>
</ul>
<p>代码框架示例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ImageEncoder</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vit_type=<span class="string">"base"</span></span>):</span><br><span class="line">        ...</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, images</span>):</span><br><span class="line">        <span class="comment"># 返回: CLS特征, patch特征序列</span></span><br><span class="line">        <span class="keyword">return</span> cls_feat, patch_feats</span><br></pre></td></tr></table></figure>
<h4 id="2-Text-Transformer"><a href="#2-Text-Transformer" class="headerlink" title="(2) Text Transformer"></a>(2) Text Transformer</h4><p><strong>必须支持：</strong></p>
<ul>
<li><strong>参数共享</strong>：embedding、transformer blocks统一</li>
<li><strong>Self-Attention类型切换</strong>：<ul>
<li>Encoder模式 → 双向mask</li>
<li>Decoder模式 → 因果mask</li>
</ul>
</li>
<li><strong>Cross-Attention可选</strong>：ITC不启用，ITM/LM启用</li>
<li><strong>特殊token机制</strong>：<ul>
<li><code>[CLS]</code>用于ITC全局特征</li>
<li><code>[ENC]</code>用于ITM分类</li>
<li><code>[DEC]</code>用于LM生成起始点</li>
</ul>
</li>
</ul>
<p>伪代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MEDTransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_dim, num_heads</span>):</span><br><span class="line">        self.self_attn = MultiHeadAttention(...)</span><br><span class="line">        self.cross_attn = MultiHeadAttention(...)</span><br><span class="line">        self.ffn = FeedForward(...)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, image_feats=<span class="literal">None</span>, mode=<span class="string">"itc"</span></span>):</span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">"itc"</span>:  <span class="comment"># ITC</span></span><br><span class="line">            x = self.self_attn(x, mask=<span class="literal">None</span>)</span><br><span class="line">            <span class="comment"># 不启用 cross-attn</span></span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">"itm"</span>:  <span class="comment"># ITM</span></span><br><span class="line">            x = self.self_attn(x, mask=<span class="literal">None</span>)</span><br><span class="line">            x = self.cross_attn(x, kv=image_feats)</span><br><span class="line">        <span class="keyword">elif</span> mode == <span class="string">"lm"</span>:  <span class="comment"># LM</span></span><br><span class="line">            x = self.self_attn(x, mask=causal_mask)</span><br><span class="line">            x = self.cross_attn(x, kv=image_feats)</span><br><span class="line">        x = self.ffn(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h4 id="3-三种模式对应特征"><a href="#3-三种模式对应特征" class="headerlink" title="(3) 三种模式对应特征"></a>(3) 三种模式对应特征</h4><div class="table-container">
<table>
<thead>
<tr>
<th>模式</th>
<th>Text起始token</th>
<th>Self-Attention</th>
<th>Cross-Attention</th>
<th>输出</th>
</tr>
</thead>
<tbody>
<tr>
<td>ITC</td>
<td><code>[CLS]</code></td>
<td>Bi-SA</td>
<td>✗</td>
<td><code>[CLS]</code> embedding → contrastive head</td>
</tr>
<tr>
<td>ITM</td>
<td><code>[ENC]</code></td>
<td>Bi-SA</td>
<td>✓</td>
<td><code>[ENC]</code> embedding → classifier</td>
</tr>
<tr>
<td>LM</td>
<td><code>[DEC]</code></td>
<td>Causal-SA</td>
<td>✓</td>
<td>token概率分布</td>
</tr>
</tbody>
</table>
</div>
<h3 id="一些细节"><a href="#一些细节" class="headerlink" title="一些细节"></a>一些细节</h3><ol>
<li><strong>forward时根据token判断模式</strong>（而不是写三份模型）。</li>
<li><strong>mask逻辑必须动态生成</strong>，保证Encoder与Decoder共享block。</li>
<li><strong>同一batch支持多任务样本</strong> → 输入序列混合 <code>[CLS]</code>/<code>[ENC]</code>/<code>[DEC]</code>，loss分支根据起始token决定。</li>
<li><strong>图像特征缓存</strong>：同一图像在ITC、ITM、LM三个loss中共用一次ViT输出，节省计算。</li>
</ol>
<h2 id="3-数据方法：CapFilt"><a href="#3-数据方法：CapFilt" class="headerlink" title="3. 数据方法：CapFilt"></a>3. 数据方法：CapFilt</h2><p>VLP面对的一个数据瓶颈是：</p>
<ul>
<li>大量来自Web的图文对（image-text pairs）<strong>质量低</strong>：文本经常只是文件名、短标签或者与图像内容无关。</li>
<li>如果直接拿这些噪声数据去做预训练，模型的对齐能力（ITC、ITM）会受到严重影响。</li>
</ul>
<p>所以作者希望：</p>
<ul>
<li>不依赖大量人工标注，也能<strong>自动生成更干净、更丰富、更有信息量的训练数据</strong>。</li>
</ul>
<p>BLIP在数据层面的核心创新是<strong>Captioning + Filtering (CapFilt)</strong>：</p>
<p><img src="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250427234034408.png" alt=""></p>
<p>CapFilt是 <strong>“Captioning + Filtering”</strong> 的缩写，它不是单一步骤，而是一个<strong>两阶段的数据自举（bootstrapping）策略</strong>：</p>
<ol>
<li><strong>Captioning (Cap)：生成更优质的文本描述</strong><ul>
<li>用一个<strong>初始的Captioner模型</strong>（由BLIP MED结构fine-tune得到）给web图片生成新的caption（synthetic caption）。</li>
<li>生成策略使用 <strong>nucleus sampling</strong>（top-p采样，p=0.9），不是beam search。<ul>
<li><strong>原因：</strong>nucleus sampling生成的句子更有多样性 → 更好的泛化性，即便包含少量噪声，也比“安全但重复”的beam search强。</li>
</ul>
</li>
</ul>
</li>
<li><strong>Filtering (Filt)：过滤无关或低质量文本</strong><ul>
<li>用一个<strong>Filter模型</strong>（也是BLIP结构fine-tune得到），计算image-text对的<strong>匹配分数 (ITM score)</strong>。</li>
<li>根据阈值筛掉不相关的web原始文本和生成的synthetic captions。</li>
</ul>
</li>
</ol>
<p><img src="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428170422257.png" alt=""></p>
<p>它们不是直接在噪声web数据上训练的。</p>
<ul>
<li>它们先由<strong>已经预训练好的BLIP模型参数初始化</strong>。</li>
<li>然后用<strong>COCO等人工标注的高质量数据</strong>做轻量微调：<ul>
<li>Captioner：在COCO caption任务上fine-tune → 学习生成高质量描述。</li>
<li>Filter：在COCO ITM任务上fine-tune → 学习判断图像与文本是否匹配。</li>
</ul>
</li>
</ul>
<p>这样保证了初始Captioner和Filter的质量足够高，不会完全被web噪声带偏。</p>
<p>最终得到：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Bootstrapped Dataset =</span><br><span class="line">    人工标注对（高质量、数量少） +</span><br><span class="line">    过滤后的web原始文本对 Tw'（噪声降低） +</span><br><span class="line">    过滤后的synthetic captions对 Ts'（新增信息、更丰富）</span><br></pre></td></tr></table></figure>
<p><strong>特点：</strong></p>
<ul>
<li>数量：比COCO大得多，比原始web数据更干净。</li>
<li>质量：既有人工标注的准确性，又有synthetic captions带来的多样性。</li>
</ul>
<p>用这个新的数据集去<strong>重新预训练BLIP主模型</strong>，性能显著提升。</p>
<p><img src="https://gnnclub-1311496010.cos.ap-beijing.myqcloud.com/wp-content/uploads/2025/04/20250428173118623.png" alt=""></p>
<hr>
<h2 id="4-预训练目标"><a href="#4-预训练目标" class="headerlink" title="4. 预训练目标"></a>4. 预训练目标</h2><p>BLIP联合优化三个loss：</p>
<ul>
<li><strong>ITC (Image-Text Contrastive Loss)</strong> → 对齐视觉和语言特征空间。</li>
<li><strong>ITM (Image-Text Matching Loss)</strong> → 学习细粒度匹配关系。</li>
<li><strong>LM (Language Modeling Loss)</strong> → 生成文本，提高跨模态生成能力。</li>
</ul>
<h3 id="1-Image-Text-Contrastive-Loss-ITC"><a href="#1-Image-Text-Contrastive-Loss-ITC" class="headerlink" title="(1) Image-Text Contrastive Loss (ITC)"></a>(1) Image-Text Contrastive Loss (ITC)</h3><p>激活的模型部分如下：</p>
<ul>
<li><strong>Image Encoder (ViT)</strong> → 提取图像特征</li>
<li><strong>Text Encoder (Bi-directional Transformer)</strong> → 提取文本特征</li>
<li>不涉及cross-attention，不生成文本。</li>
</ul>
<p>假设batch大小为 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: 0;" xmlns="http://www.w3.org/2000/svg" width="2.009ex" height="1.545ex" role="img" focusable="false" viewBox="0 -683 888 683"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D441" d="M234 637Q231 637 226 637Q201 637 196 638T191 649Q191 676 202 682Q204 683 299 683Q376 683 387 683T401 677Q612 181 616 168L670 381Q723 592 723 606Q723 633 659 637Q635 637 635 648Q635 650 637 660Q641 676 643 679T653 683Q656 683 684 682T767 680Q817 680 843 681T873 682Q888 682 888 672Q888 650 880 642Q878 637 858 637Q787 633 769 597L620 7Q618 0 599 0Q585 0 582 2Q579 5 453 305L326 604L261 344Q196 88 196 79Q201 46 268 46H278Q284 41 284 38T282 19Q278 6 272 0H259Q228 2 151 2Q123 2 100 2T63 2T46 1Q31 1 31 10Q31 14 34 26T39 40Q41 46 62 46Q130 49 150 85Q154 91 221 362L289 634Q287 635 234 637Z"></path></g></g></g></svg></mjx-container>，图像特征为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="7.256ex" height="2.288ex" role="img" focusable="false" viewBox="0 -853.7 3207.2 1011.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mi" transform="translate(518,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(1089.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(2034.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(755,363) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>，文本特征为<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.357ex;" xmlns="http://www.w3.org/2000/svg" width="6.976ex" height="2.288ex" role="img" focusable="false" viewBox="0 -853.7 3083.2 1011.5"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mi" transform="translate(394,-150) scale(0.707)"><path data-c="1D456" d="M184 600Q184 624 203 642T247 661Q265 661 277 649T290 619Q290 596 270 577T226 557Q211 557 198 567T184 600ZM21 287Q21 295 30 318T54 369T98 420T158 442Q197 442 223 419T250 357Q250 340 236 301T196 196T154 83Q149 61 149 51Q149 26 166 26Q175 26 185 29T208 43T235 78T260 137Q263 149 265 151T282 153Q302 153 302 143Q302 135 293 112T268 61T223 11T161 -11Q129 -11 102 10T74 74Q74 91 79 106T122 220Q160 321 166 341T173 380Q173 404 156 404H154Q124 404 99 371T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Z"></path></g></g><g data-mml-node="mo" transform="translate(965.7,0)"><path data-c="2208" d="M84 250Q84 372 166 450T360 539Q361 539 377 539T419 540T469 540H568Q583 532 583 520Q583 511 570 501L466 500Q355 499 329 494Q280 482 242 458T183 409T147 354T129 306T124 272V270H568Q583 262 583 250T568 230H124V228Q124 207 134 177T167 112T231 48T328 7Q355 1 466 0H570Q583 -10 583 -20Q583 -32 568 -40H471Q464 -40 446 -40T417 -41Q262 -41 172 45Q84 127 84 250Z"></path></g><g data-mml-node="msup" transform="translate(1910.5,0)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="211D" d="M17 665Q17 672 28 683H221Q415 681 439 677Q461 673 481 667T516 654T544 639T566 623T584 607T597 592T607 578T614 565T618 554L621 548Q626 530 626 497Q626 447 613 419Q578 348 473 326L455 321Q462 310 473 292T517 226T578 141T637 72T686 35Q705 30 705 16Q705 7 693 -1H510Q503 6 404 159L306 310H268V183Q270 67 271 59Q274 42 291 38Q295 37 319 35Q344 35 353 28Q362 17 353 3L346 -1H28Q16 5 16 16Q16 35 55 35Q96 38 101 52Q106 60 106 341T101 632Q95 645 55 648Q17 648 17 665ZM241 35Q238 42 237 45T235 78T233 163T233 337V621L237 635L244 648H133Q136 641 137 638T139 603T141 517T141 341Q141 131 140 89T134 37Q133 36 133 35H241ZM457 496Q457 540 449 570T425 615T400 634T377 643Q374 643 339 648Q300 648 281 635Q271 628 270 610T268 481V346H284Q327 346 375 352Q421 364 439 392T457 496ZM492 537T492 496T488 427T478 389T469 371T464 361Q464 360 465 360Q469 360 497 370Q593 400 593 495Q593 592 477 630L457 637L461 626Q474 611 488 561Q492 537 492 496ZM464 243Q411 317 410 317Q404 317 401 315Q384 315 370 312H346L526 35H619L606 50Q553 109 464 243Z"></path></g></g><g data-mml-node="mi" transform="translate(755,363) scale(0.707)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g></g></g></g></svg></mjx-container>，<br> 使用温度参数 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.029ex;" xmlns="http://www.w3.org/2000/svg" width="1.17ex" height="1.005ex" role="img" focusable="false" viewBox="0 -431 517 444"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D70F" d="M39 284Q18 284 18 294Q18 301 45 338T99 398Q134 425 164 429Q170 431 332 431Q492 431 497 429Q517 424 517 402Q517 388 508 376T485 360Q479 358 389 358T299 356Q298 355 283 274T251 109T233 20Q228 5 215 -4T186 -13Q153 -13 153 20V30L203 192Q214 228 227 272T248 336L254 357Q254 358 208 358Q206 358 197 358T183 359Q105 359 61 295Q56 287 53 286T39 284Z"></path></g></g></g></svg></mjx-container> 控制softmax平滑度。</p>
<ul>
<li>图像→文本方向的对比损失：</li>
</ul>
<script type="math/tex; mode=display">
L^{\text{img}\rightarrow\text{txt}}_{ITC} = -\frac{1}{N}\sum_{i=1}^N \log \frac{\exp(\text{sim}(v_i, t_i)/\tau)}{\sum_{j=1}^N \exp(\text{sim}(v_i, t_j)/\tau)}</script><ul>
<li>文本→图像方向类似，最终ITC loss为二者平均。</li>
</ul>
<p><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.566ex;" xmlns="http://www.w3.org/2000/svg" width="8.085ex" height="2.262ex" role="img" focusable="false" viewBox="0 -750 3573.7 1000"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mtext"><path data-c="73" d="M295 316Q295 356 268 385T190 414Q154 414 128 401Q98 382 98 349Q97 344 98 336T114 312T157 287Q175 282 201 278T245 269T277 256Q294 248 310 236T342 195T359 133Q359 71 321 31T198 -10H190Q138 -10 94 26L86 19L77 10Q71 4 65 -1L54 -11H46H42Q39 -11 33 -5V74V132Q33 153 35 157T45 162H54Q66 162 70 158T75 146T82 119T101 77Q136 26 198 26Q295 26 295 104Q295 133 277 151Q257 175 194 187T111 210Q75 227 54 256T33 318Q33 357 50 384T93 424T143 442T187 447H198Q238 447 268 432L283 424L292 431Q302 440 314 448H322H326Q329 448 335 442V310L329 304H301Q295 310 295 316Z"></path><path data-c="69" d="M69 609Q69 637 87 653T131 669Q154 667 171 652T188 609Q188 579 171 564T129 549Q104 549 87 564T69 609ZM247 0Q232 3 143 3Q132 3 106 3T56 1L34 0H26V46H42Q70 46 91 49Q100 53 102 60T104 102V205V293Q104 345 102 359T88 378Q74 385 41 385H30V408Q30 431 32 431L42 432Q52 433 70 434T106 436Q123 437 142 438T171 441T182 442H185V62Q190 52 197 50T232 46H255V0H247Z" transform="translate(394,0)"></path><path data-c="6D" d="M41 46H55Q94 46 102 60V68Q102 77 102 91T102 122T103 161T103 203Q103 234 103 269T102 328V351Q99 370 88 376T43 385H25V408Q25 431 27 431L37 432Q47 433 65 434T102 436Q119 437 138 438T167 441T178 442H181V402Q181 364 182 364T187 369T199 384T218 402T247 421T285 437Q305 442 336 442Q351 442 364 440T387 434T406 426T421 417T432 406T441 395T448 384T452 374T455 366L457 361L460 365Q463 369 466 373T475 384T488 397T503 410T523 422T546 432T572 439T603 442Q729 442 740 329Q741 322 741 190V104Q741 66 743 59T754 49Q775 46 803 46H819V0H811L788 1Q764 2 737 2T699 3Q596 3 587 0H579V46H595Q656 46 656 62Q657 64 657 200Q656 335 655 343Q649 371 635 385T611 402T585 404Q540 404 506 370Q479 343 472 315T464 232V168V108Q464 78 465 68T468 55T477 49Q498 46 526 46H542V0H534L510 1Q487 2 460 2T422 3Q319 3 310 0H302V46H318Q379 46 379 62Q380 64 380 200Q379 335 378 343Q372 371 358 385T334 402T308 404Q263 404 229 370Q202 343 195 315T187 232V168V108Q187 78 188 68T191 55T200 49Q221 46 249 46H265V0H257L234 1Q210 2 183 2T145 3Q42 3 33 0H25V46H41Z" transform="translate(672,0)"></path></g><g data-mml-node="mo" transform="translate(1505,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path></g><g data-mml-node="mi" transform="translate(1894,0)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g><g data-mml-node="mo" transform="translate(2379,0)"><path data-c="2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path></g><g data-mml-node="mi" transform="translate(2823.7,0)"><path data-c="1D461" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path></g><g data-mml-node="mo" transform="translate(3184.7,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></g></g></g></svg></mjx-container> 一般是cosine similarity。</p>
<h3 id="2-Image-Text-Matching-Loss-ITM"><a href="#2-Image-Text-Matching-Loss-ITM" class="headerlink" title="(2) Image-Text Matching Loss (ITM)"></a>(2) Image-Text Matching Loss (ITM)</h3><p>激活的模型部分如下：</p>
<ul>
<li><strong>Image Encoder + Image-grounded Text Encoder</strong></li>
<li>在文本encoder内部增加 <strong>Cross-Attention (CA)</strong>，让文本特征能直接融合视觉特征。</li>
<li>输出的[Encode] token经过一个 <strong>线性分类头</strong>，预测该对是否匹配。</li>
</ul>
<p>这是一个二分类任务，损失如下：</p>
<script type="math/tex; mode=display">
L_{ITM} = -\mathbb{E}_{(I,T)\sim D} \left[y \cdot \log p_\theta(y=1|I,T) + (1-y)\log p_\theta(y=0|I,T)\right]</script><p>其中<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.257ex" height="1.971ex" role="img" focusable="false" viewBox="0 -666 2323.6 871"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1823.6,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></g></g></g></svg></mjx-container>表示图像-文本匹配，<mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.464ex;" xmlns="http://www.w3.org/2000/svg" width="5.257ex" height="1.971ex" role="img" focusable="false" viewBox="0 -666 2323.6 871"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mi"><path data-c="1D466" d="M21 287Q21 301 36 335T84 406T158 442Q199 442 224 419T250 355Q248 336 247 334Q247 331 231 288T198 191T182 105Q182 62 196 45T238 27Q261 27 281 38T312 61T339 94Q339 95 344 114T358 173T377 247Q415 397 419 404Q432 431 462 431Q475 431 483 424T494 412T496 403Q496 390 447 193T391 -23Q363 -106 294 -155T156 -205Q111 -205 77 -183T43 -117Q43 -95 50 -80T69 -58T89 -48T106 -45Q150 -45 150 -87Q150 -107 138 -122T115 -142T102 -147L99 -148Q101 -153 118 -160T152 -167H160Q177 -167 186 -165Q219 -156 247 -127T290 -65T313 -9T321 21L315 17Q309 13 296 6T270 -6Q250 -11 231 -11Q185 -11 150 11T104 82Q103 89 103 113Q103 170 138 262T173 379Q173 380 173 381Q173 390 173 393T169 400T158 404H154Q131 404 112 385T82 344T65 302T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mo" transform="translate(767.8,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mn" transform="translate(1823.6,0)"><path data-c="30" d="M96 585Q152 666 249 666Q297 666 345 640T423 548Q460 465 460 320Q460 165 417 83Q397 41 362 16T301 -15T250 -22Q224 -22 198 -16T137 16T82 83Q39 165 39 320Q39 494 96 585ZM321 597Q291 629 250 629Q208 629 178 597Q153 571 145 525T137 333Q137 175 145 125T181 46Q209 16 250 16Q290 16 318 46Q347 76 354 130T362 333Q362 478 354 524T321 597Z"></path></g></g></g></svg></mjx-container>表示不匹配。<br>负样本使用<strong>hard negative mining</strong>：选择和图像最相似但实际上不对应的文本。</p>
<h3 id="3-Language-Modeling-Loss-LM"><a href="#3-Language-Modeling-Loss-LM" class="headerlink" title="(3) Language Modeling Loss (LM)"></a>(3) Language Modeling Loss (LM)</h3><p>激活的模型部分如下：</p>
<ul>
<li><strong>Image Encoder + Image-grounded Text Decoder</strong></li>
<li>Text Decoder与Encoder共享embedding、cross-attention、FFN层，只是：<ul>
<li>自注意力从双向(Bi-SA)换成因果(Causal-SA)，保证自回归生成。</li>
</ul>
</li>
</ul>
<p>采用标准的自回归交叉熵</p>
<script type="math/tex; mode=display">
L_{LM} = -\sum_{t=1}^M \log p_\theta(w_t|w_{<t}, I)</script><h3 id="4-三个Loss联合训练"><a href="#4-三个Loss联合训练" class="headerlink" title="(4) 三个Loss联合训练"></a>(4) 三个Loss联合训练</h3><p>一次训练的forward pass：</p>
<ol>
<li><strong>Image Encoder</strong>：图像只计算一次特征。</li>
<li><strong>Text Transformer</strong>：同一组权重执行三次forward（不同mask/模式），得到：<ul>
<li>ITC分支</li>
<li>ITM分支</li>
<li>LM分支</li>
</ul>
</li>
</ol>
<p>总损失：</p>
<script type="math/tex; mode=display">
L = \lambda_{ITC} L_{ITC} + \lambda_{ITM} L_{ITM} + \lambda_{LM} L_{LM}</script><p>论文中默认权重相等。</p>
<hr>
<h2 id="5-实验结果"><a href="#5-实验结果" class="headerlink" title="5. 实验结果"></a>5. 实验结果</h2><ul>
<li>在<strong>检索、图像描述、VQA、NLVR2、VisDial</strong>等任务上均取得SOTA。</li>
<li>使用CapFilt + ViT-B (129M数据) 就能超越SimVLM (1.8B数据)，说明<strong>数据质量 &gt; 数据量</strong>。</li>
<li>Zero-shot迁移到视频-语言任务也有很强表现。</li>
</ul>
<hr>
<h2 id="6-我的理解"><a href="#6-我的理解" class="headerlink" title="6. 我的理解"></a>6. 我的理解</h2><ul>
<li><strong>MED的核心点</strong>：用一套共享Transformer结构，通过mask策略实现encoder/decoder切换，提高了参数效率与任务统一性。</li>
<li><strong>CapFilt的价值</strong>：先用少量高质量数据训练一个“会说话、会判断”的模型，再让它生成和筛选大量web数据，形成自举循环。</li>
<li><strong>设计哲学</strong>：不仅在模型上创新，更重视数据端优化，这是BLIP相比于单纯“堆数据/堆参数”的方法更有意思的地方。</li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/BLIP/" rel="tag"># BLIP</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2025/07/26/MiniViLT%EF%BC%9A%E7%AE%80%E5%8C%96ViLT%E5%AE%9E%E7%8E%B0/" rel="prev" title="MiniViLT：简化ViLT实现">
                  <i class="fa fa-angle-left"></i> MiniViLT：简化ViLT实现
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/31/FLAVA%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%EF%BC%9AFLAVA-A-Foundational-Language-And-Vision-Alignment-Model/" rel="next" title="FLAVA论文阅读笔记：FLAVA: A Foundational Language And Vision Alignment Model">
                  FLAVA论文阅读笔记：FLAVA: A Foundational Language And Vision Alignment Model <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023-12 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">千顷QianQing</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/qianqing26" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"version":"7.1.2","theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.min.js","integrity":"sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
