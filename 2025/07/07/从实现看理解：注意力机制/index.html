<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="“We call our model the Transformer. The model architecture is shown in Figure 1.”—— Attention is All You Need, Vaswani et al., 2017  在 Transformer 中，多头注意力机制（Multi-Head Attention, MHA）是核心组件之一。论文中虽然只用了">
<meta property="og:type" content="article">
<meta property="og:title" content="从实现看理解：多头注意力机制">
<meta property="og:url" content="http://example.com/2025/07/07/%E4%BB%8E%E5%AE%9E%E7%8E%B0%E7%9C%8B%E7%90%86%E8%A7%A3%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/index.html">
<meta property="og:site_name" content="QianQing&#39;s Blog">
<meta property="og:description" content="“We call our model the Transformer. The model architecture is shown in Figure 1.”—— Attention is All You Need, Vaswani et al., 2017  在 Transformer 中，多头注意力机制（Multi-Head Attention, MHA）是核心组件之一。论文中虽然只用了">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2025-07-07T15:07:30.000Z">
<meta property="article:modified_time" content="2025-07-09T00:41:30.161Z">
<meta property="article:author" content="千顷QianQing">
<meta property="article:tag" content="transformer">
<meta property="article:tag" content="注意力">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://example.com/2025/07/07/%E4%BB%8E%E5%AE%9E%E7%8E%B0%E7%9C%8B%E7%90%86%E8%A7%A3%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"http://example.com/2025/07/07/%E4%BB%8E%E5%AE%9E%E7%8E%B0%E7%9C%8B%E7%90%86%E8%A7%A3%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/","path":"2025/07/07/从实现看理解：注意力机制/","title":"从实现看理解：多头注意力机制"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>从实现看理解：多头注意力机制 | QianQing's Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">QianQing's Blog</p>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E6%AE%B5%E8%87%AA%E5%B7%B1%E7%9A%84%E5%AE%9E%E7%8E%B0"><span class="nav-number">1.</span> <span class="nav-text">一段自己的实现</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E4%B8%89%E7%A7%8D-Mask-%E7%9A%84%E8%AF%AD%E4%B9%89%E8%A7%92%E8%89%B2"><span class="nav-number">2.</span> <span class="nav-text">一、三种 Mask 的语义角色</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A4%BA%E4%BE%8B%E4%BB%A3%E7%A0%81%EF%BC%9A"><span class="nav-number">2.1.</span> <span class="nav-text">示例代码：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%8C%E3%80%81softmax-%E5%89%8D%E7%9A%84%E9%99%A4%E6%A0%B9%E5%8F%B7-d%E2%82%96-%E7%9A%84%E5%BF%85%E8%A6%81%E6%80%A7"><span class="nav-number">3.</span> <span class="nav-text">二、softmax 前的除根号 dₖ 的必要性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%89%E3%80%81masked-fill-%E4%B8%8Esoftmax%E6%95%B0%E5%80%BC%E7%A8%B3%E5%AE%9A%E6%80%A7"><span class="nav-number">4.</span> <span class="nav-text">三、masked_fill 与softmax数值稳定性</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9B%E3%80%81%E6%A8%A1%E5%9D%97%E5%8C%96%E8%AE%BE%E8%AE%A1%E4%B8%AD%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%8A%BD%E8%B1%A1"><span class="nav-number">5.</span> <span class="nav-text">四、模块化设计中的注意力抽象</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BA%94%E3%80%81%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%9A%84%E7%BB%B4%E5%BA%A6%E8%AE%BE%E8%AE%A1"><span class="nav-number">6.</span> <span class="nav-text">五、多头注意力的维度设计</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E5%A4%9A%E5%A4%B4%EF%BC%9F"><span class="nav-number">6.1.</span> <span class="nav-text">为什么要多头？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%B4%E5%BA%A6%E8%AE%BE%E8%AE%A1%E7%A4%BA%E6%84%8F"><span class="nav-number">6.2.</span> <span class="nav-text">维度设计示意</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0%EF%BC%9A"><span class="nav-number">6.3.</span> <span class="nav-text">代码实现：</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%98%E7%82%B9%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-number">6.4.</span> <span class="nav-text">优点总结：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%B0%8F%E6%80%BB%E7%BB%93"><span class="nav-number">7.</span> <span class="nav-text">小总结</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="千顷QianQing"
      src="/images/touxiang.jpg">
  <p class="site-author-name" itemprop="name">千顷QianQing</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">89</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">16</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/qianqing26" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;qianqing26" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:eevee_hbc@outlook.com" title="E-Mail → mailto:eevee_hbc@outlook.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="http://example.com/2025/07/07/%E4%BB%8E%E5%AE%9E%E7%8E%B0%E7%9C%8B%E7%90%86%E8%A7%A3%EF%BC%9A%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/touxiang.jpg">
      <meta itemprop="name" content="千顷QianQing">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="QianQing's Blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="从实现看理解：多头注意力机制 | QianQing's Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          从实现看理解：多头注意力机制
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2025-07-07 23:07:30" itemprop="dateCreated datePublished" datetime="2025-07-07T23:07:30+08:00">2025-07-07</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2025-07-09 08:41:30" itemprop="dateModified" datetime="2025-07-09T08:41:30+08:00">2025-07-09</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">In</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/%E7%A0%94%E9%9B%B6%E5%AD%A6%E4%B9%A0%E8%AE%B0%E5%BD%95/" itemprop="url" rel="index"><span itemprop="name">研零学习记录</span></a>
        </span>
    </span>

  
    <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv">
      <span class="post-meta-item-icon">
        <i class="far fa-eye"></i>
      </span>
      <span class="post-meta-item-text">Views: </span>
      <span id="busuanzi_value_page_pv"></span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><blockquote>
<p>“We call our model the Transformer. The model architecture is shown in Figure 1.”<br>—— Attention is All You Need, Vaswani et al., 2017</p>
</blockquote>
<p>在 Transformer 中，多头注意力机制（Multi-Head Attention, MHA）是核心组件之一。论文中虽然只用了短短几页进行描述，但其实现中蕴含着大量工程智慧与数学原则。本文将结合我自己复现的transformer和 GPT 模型，<strong>站在一个研零初学者的视角上</strong>，从 <strong>mask 使用</strong>、<strong>softmax 数值稳定性</strong> 角度，学习《Attention is All You Need》中的一些小细节，并结合实际代码加以分析。</p>
<span id="more"></span>
<h2 id="一段自己的实现"><a href="#一段自己的实现" class="headerlink" title="一段自己的实现"></a>一段自己的实现</h2><p>先来看一段我的实现代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">scores = (Q @ K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)</span><br><span class="line">scores = scores.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">"-inf"</span>))</span><br><span class="line">attn = torch.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">attn = self.dropout(attn)</span><br></pre></td></tr></table></figure>
<p>这几行代码常见于多头注意力实现，其背后隐含的思路与原始论文的公式（如下）完全一致：</p>
<script type="math/tex; mode=display">
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V</script><blockquote>
<p>cite：论文公式 (1)</p>
</blockquote>
<hr>
<h2 id="一、三种-Mask-的语义角色"><a href="#一、三种-Mask-的语义角色" class="headerlink" title="一、三种 Mask 的语义角色"></a>一、三种 Mask 的语义角色</h2><p>论文中在描述 Decoder 时提到：</p>
<blockquote>
<p>“To prevent leftward information flow in the decoder to preserve the auto-regressive property, we apply a mask to the input…” </p>
</blockquote>
<p>这说明在 Decoder 的 <code>Self-Attention</code> 中，mask 是结构必要条件。在工程中，我们通常使用三种 mask：</p>
<ol>
<li><strong>Padding mask</strong>：忽略 <code>&lt;PAD&gt;</code> 的影响；</li>
<li><strong>Look-ahead mask</strong>：防止看到未来的 token；</li>
<li><strong>组合 mask</strong>：前两种的结合。</li>
</ol>
<h3 id="示例代码："><a href="#示例代码：" class="headerlink" title="示例代码："></a>示例代码：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = scores.masked_fill(~mask, <span class="built_in">float</span>(<span class="string">"-inf"</span>))</span><br></pre></td></tr></table></figure>
<p>这相当于把不应被注意的位置设为 <code>-∞</code>，使其 softmax 权重为 0。</p>
<p>注意：mask 的 shape 通常应为 <code>[batch, 1, 1, seq_len]</code> 或 <code>[batch, head, seq_len, seq_len]</code>，否则将 silently fail！</p>
<hr>
<h2 id="二、softmax-前的除根号-dₖ-的必要性"><a href="#二、softmax-前的除根号-dₖ-的必要性" class="headerlink" title="二、softmax 前的除根号 dₖ 的必要性"></a>二、softmax 前的除根号 dₖ 的必要性</h2><blockquote>
<p>“This scaling is necessary to counteract the effect of dot products growing large in magnitude…” </p>
</blockquote>
<p>随着维度增长，dot-product 越来越大，softmax 输出趋于极端（接近 one-hot），梯度消失。因此我们除以 <mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -0.372ex;" xmlns="http://www.w3.org/2000/svg" width="4.128ex" height="2.398ex" role="img" focusable="false" viewBox="0 -895.6 1824.4 1060"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msqrt"><g transform="translate(853,0)"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g></g><g data-mml-node="mo" transform="translate(0,35.6)"><path data-c="221A" d="M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"></path></g><rect width="971.4" height="60" x="853" y="775.6"></rect></g></g></g></svg></mjx-container>，使得 softmax 有效范围保持稳定。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scores = (Q @ K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)</span><br></pre></td></tr></table></figure>
<p> 亲测若省略这一步，模型学习将大幅恶化，训练过程可能出现 loss 为 NaN。</p>
<hr>
<h2 id="三、masked-fill-与softmax数值稳定性"><a href="#三、masked-fill-与softmax数值稳定性" class="headerlink" title="三、masked_fill 与softmax数值稳定性"></a>三、masked_fill 与softmax数值稳定性</h2><p>从数值计算角度讲：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">attn = torch.softmax(scores, dim=-<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>其中 <code>scores</code> 已被设为 <code>-inf</code> 的位置，在 softmax 中自动转化为 0。若设为较小负值（如 -1e9），虽然理论上也趋近 0，但会留下数值误差，甚至导致梯度传播问题。</p>
<p> 因此，最好用 <code>float('-inf')</code>，PyTorch 内部对此进行了优化。</p>
<hr>
<h2 id="四、模块化设计中的注意力抽象"><a href="#四、模块化设计中的注意力抽象" class="headerlink" title="四、模块化设计中的注意力抽象"></a>四、模块化设计中的注意力抽象</h2><p>通过模块化设计，我们可以将注意力机制抽象为如下计算流程：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Q, K, V -&gt; attention score -&gt; mask -&gt; softmax -&gt; dropout -&gt; context vector</span><br></pre></td></tr></table></figure>
<p>这样的设计与论文逻辑对齐，也便于复用不同注意力策略（如 Sparse Attention、Causal Attention、Relative Position Attention 等）。</p>
<hr>
<h2 id="五、多头注意力的维度设计"><a href="#五、多头注意力的维度设计" class="headerlink" title="五、多头注意力的维度设计"></a>五、多头注意力的维度设计</h2><blockquote>
<p>“Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections…”</p>
</blockquote>
<hr>
<h3 id="为什么要多头？"><a href="#为什么要多头？" class="headerlink" title="为什么要多头？"></a>为什么要多头？</h3><p>Transformer 并不使用一个大头（单个全维度 attention），而是将 <code>d_model</code> 拆分成多个小头，每个头可以关注输入的不同部分。这一设计背后有三个动因：</p>
<ol>
<li><strong>增强表达能力</strong>：每个注意力头可以学习不同的子空间；</li>
<li><strong>并行高效计算</strong>：所有头共享相同的输入，但使用不同投影；</li>
<li><strong>更少计算开销</strong>：小头维度 <code>d_k = d_model / n_heads</code> 降低了 softmax 操作的复杂度。</li>
</ol>
<hr>
<h3 id="维度设计示意"><a href="#维度设计示意" class="headerlink" title="维度设计示意"></a>维度设计示意</h3><div class="table-container">
<table>
<thead>
<tr>
<th>项目</th>
<th>维度（假设）</th>
<th>说明</th>
</tr>
</thead>
<tbody>
<tr>
<td>输入 <code>x</code></td>
<td><code>[B, T, d_model]</code></td>
<td>Batch 大小为 B，序列长度为 T</td>
</tr>
<tr>
<td>Q/K/V 投影前</td>
<td><code>[B, T, d_model]</code></td>
<td>Embedding or上一层输出</td>
</tr>
<tr>
<td>Q/K/V 投影后</td>
<td><code>[B, h, T, d_k]</code></td>
<td>多头展开后，每个头维度为 <code>d_k</code></td>
</tr>
<tr>
<td>Attention输出</td>
<td><code>[B, h, T, d_k]</code></td>
<td>每头输出维度一致</td>
</tr>
<tr>
<td>合并后输出</td>
<td><code>[B, T, d_model]</code></td>
<td>拼接所有头，再做一次线性变换</td>
</tr>
</tbody>
</table>
</div>
<p>其中：</p>
<ul>
<li><mjx-container class="MathJax" jax="SVG"><svg style="vertical-align: -1.688ex;" xmlns="http://www.w3.org/2000/svg" width="16.966ex" height="4.507ex" role="img" focusable="false" viewBox="0 -1246.1 7499.2 1992.3"><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D458" d="M121 647Q121 657 125 670T137 683Q138 683 209 688T282 694Q294 694 294 686Q294 679 244 477Q194 279 194 272Q213 282 223 291Q247 309 292 354T362 415Q402 442 438 442Q468 442 485 423T503 369Q503 344 496 327T477 302T456 291T438 288Q418 288 406 299T394 328Q394 353 410 369T442 390L458 393Q446 405 434 405H430Q398 402 367 380T294 316T228 255Q230 254 243 252T267 246T293 238T320 224T342 206T359 180T365 147Q365 130 360 106T354 66Q354 26 381 26Q429 26 459 145Q461 153 479 153H483Q499 153 499 144Q499 139 496 130Q455 -11 378 -11Q333 -11 305 15T277 90Q277 108 280 121T283 145Q283 167 269 183T234 206T200 217T182 220H180Q168 178 159 139T145 81T136 44T129 20T122 7T111 -2Q98 -11 83 -11Q66 -11 57 -1T48 16Q48 26 85 176T158 471L195 616Q196 629 188 632T149 637H144Q134 637 131 637T124 640T121 647Z"></path></g></g><g data-mml-node="mo" transform="translate(1249.2,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="msub" transform="translate(2305,0)"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(553,-150) scale(0.707)"><path data-c="1D463" d="M173 380Q173 405 154 405Q130 405 104 376T61 287Q60 286 59 284T58 281T56 279T53 278T49 278T41 278H27Q21 284 21 287Q21 294 29 316T53 368T97 419T160 441Q202 441 225 417T249 361Q249 344 246 335Q246 329 231 291T200 202T182 113Q182 86 187 69Q200 26 250 26Q287 26 319 60T369 139T398 222T409 277Q409 300 401 317T383 343T365 361T357 383Q357 405 376 424T417 443Q436 443 451 425T467 367Q467 340 455 284T418 159T347 40T241 -11Q177 -11 139 22Q102 54 102 117Q102 148 110 181T151 298Q173 362 173 380Z"></path></g></g><g data-mml-node="mo" transform="translate(3528.7,0)"><path data-c="3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path></g><g data-mml-node="mfrac" transform="translate(4584.5,0)"><g data-mml-node="mrow" transform="translate(220,552.1) scale(0.707)"><g data-mml-node="mpadded"><g data-mml-node="mrow"></g></g><g data-mml-node="mstyle" transform="scale(1.414)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="msub"><g data-mml-node="mi"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="TeXAtom" transform="translate(553,-150) scale(0.707)" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="1D45A" d="M21 287Q22 293 24 303T36 341T56 388T88 425T132 442T175 435T205 417T221 395T229 376L231 369Q231 367 232 367L243 378Q303 442 384 442Q401 442 415 440T441 433T460 423T475 411T485 398T493 385T497 373T500 364T502 357L510 367Q573 442 659 442Q713 442 746 415T780 336Q780 285 742 178T704 50Q705 36 709 31T724 26Q752 26 776 56T815 138Q818 149 821 151T837 153Q857 153 857 145Q857 144 853 130Q845 101 831 73T785 17T716 -10Q669 -10 648 17T627 73Q627 92 663 193T700 345Q700 404 656 404H651Q565 404 506 303L499 291L466 157Q433 26 428 16Q415 -11 385 -11Q372 -11 364 -4T353 8T350 18Q350 29 384 161L420 307Q423 322 423 345Q423 404 379 404H374Q288 404 229 303L222 291L189 157Q156 26 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 112 181Q151 335 151 342Q154 357 154 369Q154 405 129 405Q107 405 92 377T69 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path></g><g data-mml-node="mi" transform="translate(878,0)"><path data-c="1D45C" d="M201 -11Q126 -11 80 38T34 156Q34 221 64 279T146 380Q222 441 301 441Q333 441 341 440Q354 437 367 433T402 417T438 387T464 338T476 268Q476 161 390 75T201 -11ZM121 120Q121 70 147 48T206 26Q250 26 289 58T351 142Q360 163 374 216T388 308Q388 352 370 375Q346 405 306 405Q243 405 195 347Q158 303 140 230T121 120Z"></path></g><g data-mml-node="mi" transform="translate(1363,0)"><path data-c="1D451" d="M366 683Q367 683 438 688T511 694Q523 694 523 686Q523 679 450 384T375 83T374 68Q374 26 402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487H491Q506 153 506 145Q506 140 503 129Q490 79 473 48T445 8T417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157Q33 205 53 255T101 341Q148 398 195 420T280 442Q336 442 364 400Q369 394 369 396Q370 400 396 505T424 616Q424 629 417 632T378 637H357Q351 643 351 645T353 664Q358 683 366 683ZM352 326Q329 405 277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q233 26 290 98L298 109L352 326Z"></path></g><g data-mml-node="mi" transform="translate(1883,0)"><path data-c="1D452" d="M39 168Q39 225 58 272T107 350T174 402T244 433T307 442H310Q355 442 388 420T421 355Q421 265 310 237Q261 224 176 223Q139 223 138 221Q138 219 132 186T125 128Q125 81 146 54T209 26T302 45T394 111Q403 121 406 121Q410 121 419 112T429 98T420 82T390 55T344 24T281 -1T205 -11Q126 -11 83 42T39 168ZM373 353Q367 405 305 405Q272 405 244 391T199 357T170 316T154 280T149 261Q149 260 169 260Q282 260 327 284T373 353Z"></path></g><g data-mml-node="mi" transform="translate(2349,0)"><path data-c="1D459" d="M117 59Q117 26 142 26Q179 26 205 131Q211 151 215 152Q217 153 225 153H229Q238 153 241 153T246 151T248 144Q247 138 245 128T234 90T214 43T183 6T137 -11Q101 -11 70 11T38 85Q38 97 39 102L104 360Q167 615 167 623Q167 626 166 628T162 632T157 634T149 635T141 636T132 637T122 637Q112 637 109 637T101 638T95 641T94 647Q94 649 96 661Q101 680 107 682T179 688Q194 689 213 690T243 693T254 694Q266 694 266 686Q266 675 193 386T118 83Q118 81 118 75T117 65V59Z"></path></g></g></g></g></g></g><g data-mml-node="mrow" transform="translate(1169.4,-534) scale(0.707)"><g data-mml-node="mpadded"><g data-mml-node="mrow"></g></g><g data-mml-node="mstyle" transform="scale(1.414)"><g data-mml-node="TeXAtom" data-mjx-texclass="ORD"><g data-mml-node="mi"><path data-c="210E" d="M137 683Q138 683 209 688T282 694Q294 694 294 685Q294 674 258 534Q220 386 220 383Q220 381 227 388Q288 442 357 442Q411 442 444 415T478 336Q478 285 440 178T402 50Q403 36 407 31T422 26Q450 26 474 56T513 138Q516 149 519 151T535 153Q555 153 555 145Q555 144 551 130Q535 71 500 33Q466 -10 419 -10H414Q367 -10 346 17T325 74Q325 90 361 192T398 345Q398 404 354 404H349Q266 404 205 306L198 293L164 158Q132 28 127 16Q114 -11 83 -11Q69 -11 59 -2T48 16Q48 30 121 320L195 616Q195 629 188 632T149 637H128Q122 643 122 645T124 664Q129 683 137 683Z"></path></g></g></g></g><rect width="2674.7" height="60" x="120" y="220"></rect></g></g></g></svg></mjx-container>，如论文中设定 <code>d_model = 512</code>，<code>h = 8</code>，则每头 <code>d_k = 64</code></li>
</ul>
<hr>
<h3 id="代码实现："><a href="#代码实现：" class="headerlink" title="代码实现："></a>代码实现：</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入 [B, T, d_model]</span></span><br><span class="line">Q = self.q_proj(query)  <span class="comment"># -&gt; [B, T, d_model]</span></span><br><span class="line">K = self.k_proj(key)    <span class="comment"># -&gt; [B, T, d_model]</span></span><br><span class="line">V = self.v_proj(value)  <span class="comment"># -&gt; [B, T, d_model]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 拆成多头，变为 [B, h, T, d_k]</span></span><br><span class="line">Q = Q.view(B, T, h, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">K = K.view(B, T, h, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">V = V.view(B, T, h, d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力计算与拼接输出</span></span><br><span class="line">scores = (Q @ K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / sqrt(d_k)  <span class="comment"># [B, h, T, T]</span></span><br><span class="line">attn = softmax(scores)</span><br><span class="line">output = attn @ V  <span class="comment"># [B, h, T, d_k]</span></span><br><span class="line">output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).reshape(B, T, d_model)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="优点总结："><a href="#优点总结：" class="headerlink" title="优点总结："></a>优点总结：</h3><ul>
<li><strong>高效并行性</strong>：多头 attention 可以一次性计算所有 heads；</li>
<li><strong>低内存占用</strong>：相比直接做 <code>[B, T, T]</code> 的高维注意力，这种头部拆分方式更节省；</li>
<li><strong>性能更强</strong>：论文实验证明：多个小头的组合 &gt; 一个大头；</li>
<li><strong>语义多样性</strong>：不同头可捕捉不同的语法/语义关系，如某些关注主语，某些关注动词。</li>
</ul>
<hr>
<h2 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h2><p>Transformer 的强大源于注意力机制的全局建模能力，但真正掌握它的工程实现，必须深入每一处看似微不足道的细节——如 mask 的逻辑维度、数值稳定性、softmax 、维度设计的配合。</p>
<p>这也是我将复现模型作为学习的一部分的初衷：<strong>只有亲手书写，才能真正理解为何如此实现、为何不能省略。</strong></p>
<hr>
<p>参考文献：</p>
<ul>
<li>Vaswani et al., “Attention is All You Need”, 2017</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/transformer/" rel="tag"># transformer</a>
              <a href="/tags/%E6%B3%A8%E6%84%8F%E5%8A%9B/" rel="tag"># 注意力</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/06/18/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%EF%BC%9A%E5%88%92%E5%88%86%E6%A0%91/" rel="prev" title="数据结构：划分树">
                  <i class="fa fa-angle-left"></i> 数据结构：划分树
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2025/07/09/%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%9A%84%E7%9F%A9%E9%98%B5%E7%BB%B4%E5%BA%A6%E5%88%86%E6%9E%90/" rel="next" title="多头注意力计算中的矩阵维度分析">
                  多头注意力计算中的矩阵维度分析 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023-12 – 
    <span itemprop="copyrightYear">2025</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">千顷QianQing</span>
  </div>
<div class="busuanzi-count">
    <span class="post-meta-item" id="busuanzi_container_site_uv">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-item" id="busuanzi_container_site_pv">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>

    </div>
  </footer>

  
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/qianqing26" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/next-boot.js"></script>

  

  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"version":"7.1.2","theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mermaid/10.7.0/mermaid.min.js","integrity":"sha256-TtLOdUA8mstPoO6sGvHIGx2ceXrrX4KgIItO06XOn8A="}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>





  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","mhchem":true,"js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



</body>
</html>
